---
title: "Support Vector Machine Homework Soltuions"
author: "Alex Baransky"
output: html_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message=FALSE)
```

## Question #1: Wine Quality Data
**Purpose: Demonstrate understanding of how to a Support Vector Classification model**

1. Load: Read in the Wine Quality.csv dataset into your workspace. The data contains 1,599 observations of red Vinho Verde wines from the north of Portugal. The goal is to model wine quality based on various physicochemical measurements.

```{r}
wine = read.csv("https://s3.amazonaws.com/nycdsabt01/Wine+Quality.csv")

```

2. Munge: Perform some data munging:
    a. Recode the quality variable to be a factor variable with values of “Low” for       quality ratings of 5 and below, and “High” for ratings of 6 and above.
    b. Scale and center the numeric vectors of your dataset.

```{r}
wine$quality = as.factor(ifelse(wine$quality <= 5, 'Low', 'High'))
wine_scaled = as.data.frame(scale(subset(wine, select=-quality), center=TRUE, scale=FALSE))
wine_scaled$quality = wine$quality
```

3. Split: Split the data into a training and test set with an 80% - 20% split, respectively. (NB: Use set.seed(0) so your results will be reproducible.)

```{r}
set.seed(0)
train_index = sample(1:nrow(wine_scaled), .8*nrow(wine_scaled))
train = wine_scaled[train_index,]
test = wine_scaled[-train_index,]
```

4. Plot: Briefly explore some graphical EDA:
    a. Explain why a maximal margin classifier is impossible to fit to this data. (NB:    Do not try to fit the maximal margin classifier.)
    b. Explain why a support vector classifier is generally more desirable.

```{r}
for (name in colnames(subset(train, select = -quality))){
  plot(train[,name], col = train$quality, ylab = name)
  legend("topright", legend = c("Low", "High"), fill = c("red", "black"))
}
# a, the classes greatly overlap, there is not strict margin
# b, if we can loosen the margin requirments, we can estimate the quality of a wine
```

5. Fit a Model - Tune Cost Hyperparameter: Tune a support vector classifier with a linear kernel and cost ranging from 10-5 to 10-.5; try using the code snippet cost = 10^(seq(-5, -.5, length = 50)). Caution: This will take about a minute to run. (NB: Use set.seed(0) so your results will be reproducible.)

```{r}
library(e1071)

set.seed(0)
tune.cost = tune.svm(quality ~ ., 
                     data = train, 
                     kernel = 'linear',
                    cost = 10^(seq(-5, -.5, length = 50)))
```

6. Analyze: 
    a. What was the best cost parameter of the ones you tested?
    b. What was the best error rate corresponding to the best cost?
    c. Graphically view the cross-validated results. Is it plausible that you checked     enough values of cost?
    d. How many support vectors are there in the best support vector classifier you       found?
    e. What is the test error associated with this best support vector classifier?

```{r}
summary(tune.cost)
best.cost = tune.cost$best.parameters$cost
tune.cost$performances$error[tune.cost$performances$cost == tune.cost$best.parameters$cost]
plot(tune.cost)
tune.cost$best.model$nSV
tune.cost.test = predict(tune.cost$best.model, subset(test, select=-quality))
table(test$quality, tune.cost.test)
(38+44)/(134+44+38+104)
```

7. Fit a Model to More Data: Fit a support vector classifier to all of the data using the best cost parameter you found in part 5.
    a. How many support vectors does this support vector classifier have?
    b. Is the 555th observation a support vector?
    c. What is the overall error rate for this support vector classifier?

```{r}
all.svm = svm(quality ~ .,
              data = wine_scaled,
              kernel = 'linear',
              cost = best.cost)
sum(all.svm$nSV)
all(wine_scaled[555,] %in% all.svm$SV)
# error?
```

8. Plot: Visualize the support vector classifier by examining the free sulfur dioxide and total sulfur dioxide cross-section; to do so, use the following line of code (modified with your object names): plot(model, data, free.sulfur.dioxide ~ total.sulfur.dioxide).

```{r}
plot(all.svm, wine_scaled, free.sulfur.dioxide ~ total.sulfur.dioxide)
```

9. Fit a Model with a Radial Kernel: Tune a support vector machine with a radial kernel. Check both cost and gamma values using the following code snippets: cost = seq(.75, 1.25, length = 5), gamma = seq(.55, .95, length = 5). Caution: This will take about a minute to run. (NB: Use set.seed(0) so your results will be reproducible.)

```{r}
set.seed(0)
tune.gamma = tune.svm(quality ~ ., 
                     data = train, 
                     kernel = 'radial',
                    cost = seq(.75, 1.25, length = 5),
                    gamma = seq(.55, .95, length = 5))
```

10. Analyze: 
    a. What was the best cost parameter of the ones you tested?
    b. What was the best gamma parameter of the ones you tested?
    c. What was the best error rate corresponding to the best cost & gamma?
    d. Graphically view the cross-validated results. Is it plausible that you checked     enough values of cost and gamma?
    e. How many support vectors are there in the best support vector machine you found?
    f. What is the test error associated with this best support vector machine?

```{r}
best.cost2 = tune.gamma$best.parameters$cost
best.gamma = tune.gamma$best.parameters$gamma
tune.gamma$performances$error[tune.gamma$performances$cost == tune.gamma$best.parameters$cost &
                                tune.gamma$performances$gamma == tune.gamma$best.parameters$gamma]
plot(tune.gamma)
sum(tune.gamma$best.model$nSV)
tune.gamma.test = predict(tune.gamma$best.model, subset(test, select=-quality))
table(test$quality, tune.gamma.test)
(40+33)/(145+33+40+102)
```

11. Fit a Model to More Data: Fit a support vector machine to all of the data using the best cost and gamma parameters you found in part 9.
    a. How many support vectors does this support vector machine have?
    b. Is the 798th observation a support vector?
    c. What is the overall error rate for this support vector machine?

```{r}
all.svm2 = svm(quality ~ .,
              data = wine_scaled,
              kernel = 'radial',
              cost = best.cost2,
              gamma = best.gamma)
sum(all.svm2$nSV)
all(wine_scaled[798,] %in% all.svm2$SV)
# error?
```

12. Plot: Visualize the support vector machine by examining the free sulfur dioxide and total sulfur dioxide cross-section; to do so, use the following line of code (modified with your object names): plot(model, data, free.sulfur.dioxide ~ total.sulfur.dioxide).

```{r}
plot(all.svm2, wine_scaled, free.sulfur.dioxide ~ total.sulfur.dioxide)
```

13. Compare: List a pro and con for both:
    a. The best support vector classifier you found in part 5.
    b. The best support vector machine you found in part 9.

```{r}
# a) Captured more Low behavior but also increased error by including more High in the region
# b) Captured less Low behavior but increased the accuracy of the predictions (less High in the region)
```

## Question #2: Machine Learning Theory
**Purpose:Demonstrate Theory of Lecture Material**

1. Hyperplanes: In training a support vector machine, we use a hyperplane to separate predicted classes. 
    a. In a 3-dimensional space, how many dimensions does the separating hyperplane have?
    b. In an n-dimensional space, how many dimensions does the separating hyperplane have?
```{r}
# a) 2 dimensions
# b) n-1 dimensions
```

2. Kernels: What is one advantage of using a linear kernel as opposed to another type (e.g. radial, polynomial)?
    a. Conceptually, why does a linear mapping, relative to other types of mappings,      lead to this particular advantage?
```{r}
# a) linear kernels train and predict tests much faster than other kernels. Linear kernels are degenerate versions of RBF kernels
```

## Question #3: Challenge Question
1. Try fitting a support vector machine with a polynomial kernel. Figure out which parameters you need to tune and test out a range of possible values. 
    a. What were your best parameters? 
    b. How did this classifier perform relative to the others?
```{r}
tune.poly = tune.svm(quality ~ ., 
                     data = train, 
                     kernel = 'polynomial',
                    cost = seq(.75, 1.25, length = 5),
                    gamma = seq(.55, .95, length = 5))

tune.poly$best.parameters
tune.poly$performances$error[tune.poly$performances$cost == tune.poly$best.parameters$cost &
                                tune.poly$performances$gamma == tune.poly$best.parameters$gamma]
plot(tune.poly$best.model, train, free.sulfur.dioxide ~ total.sulfur.dioxide)
```